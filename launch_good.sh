# Run with:
# launch launch_good.sh --classical_logfile_names --gpu_type rtxa6000 --mem 30

# python main.py --max_grad_norm 0.2 --lr_scheduler_type cosine --num_generations 12 --gradient_accumulation_steps 4 --learning_rate 5e-5 --num_train_epochs 4 --model_run_name Qwen2.5-14B_7500 --model_name /fs/cml-projects/guardian_models/models/Qwen2.5-14B-Instruct/huggingface_sft/lora_7500 --gpu_memory_utilization 0.7
# python main.py --max_grad_norm 0.2 --lr_scheduler_type cosine --num_generations 12 --gradient_accumulation_steps 4 --learning_rate 5e-5 --num_train_epochs 4 --model_run_name Meta-Llama-3.1-8B_7500 --model_name /fs/cml-projects/guardian_models/models/Meta-Llama-3.1-8B-Instruct/huggingface_sft/7500

# python main.py --max_grad_norm 0.2 --lr_scheduler_type cosine --num_generations 12 --gradient_accumulation_steps 4 --learning_rate 5e-5 --num_train_epochs 4 --model_run_name Qwen2.5-14B_multirule_v2 --model_name /fs/cml-projects/guardian_models/models/Qwen2.5-14B-Instruct/huggingface_sft/lora_multirule_v2 --save_steps 50 --gpu_memory_utilization 0.7 --resume_from_checkpoint
# python main.py --max_grad_norm 0.2 --lr_scheduler_type cosine --num_generations 12 --gradient_accumulation_steps 4 --learning_rate 5e-5 --num_train_epochs 4 --model_run_name Qwen2.5-7B_multirule_v2_X --model_name /fs/cml-projects/guardian_models/models/Qwen2.5-7B-Instruct/huggingface_sft/lora_multirule_v2 --save_steps 100
# python main.py --max_grad_norm 0.2 --lr_scheduler_type cosine --num_generations 12 --gradient_accumulation_steps 4 --learning_rate 5e-5 --num_train_epochs 4 --model_run_name Qwen2.5-3B_multirule_v2 --model_name /fs/cml-projects/guardian_models/models/Qwen2.5-3B-Instruct/huggingface_sft/lora_multirule_v2   
# python main.py --max_grad_norm 0.2 --lr_scheduler_type cosine --num_generations 12 --gradient_accumulation_steps 4 --learning_rate 5e-5 --num_train_epochs 8 --model_run_name Qwen2.5-1.5B_multirule_v2 --model_name /fs/cml-projects/guardian_models/models/Qwen2.5-1.5B-Instruct/huggingface_sft/lora_multirule_v2 --resume_from_checkpoint
# python main.py --max_grad_norm 0.2 --lr_scheduler_type cosine --num_generations 12 --gradient_accumulation_steps 4 --learning_rate 5e-5 --num_train_epochs 8 --model_run_name Qwen2.5-0.5B_multirule_v2 --model_name /fs/cml-projects/guardian_models/models/Qwen2.5-0.5B-Instruct/huggingface_sft/lora_multirule_v2 --resume_from_checkpoint

# python main.py --max_grad_norm 0.2 --lr_scheduler_type cosine --num_generations 12 --gradient_accumulation_steps 4 --learning_rate 5e-5 --num_train_epochs 4 --model_run_name Meta-Llama-3.1-8B_multirule_v2 --model_name /fs/cml-projects/guardian_models/models/Meta-Llama-3.1-8B-Instruct/huggingface_sft/lora_multirule_v2 --save_steps 100

# python main.py --max_grad_norm 0.2 --lr_scheduler_type cosine --num_generations 12 --gradient_accumulation_steps 4 --learning_rate 5e-5 --num_train_epochs 4 --model_run_name LlamaGuard3-8B_7500 --model_name /fs/cml-projects/guardian_models/models/Llama-Guard-3-8B/huggingface_sft/lora_7500
# python main.py --max_grad_norm 0.2 --lr_scheduler_type cosine --num_generations 12 --gradient_accumulation_steps 4 --learning_rate 5e-5 --num_train_epochs 4 --model_run_name LlamaGuard3-8B_multirule_v2 --model_name /fs/cml-projects/guardian_models/models/Llama-Guard-3-8B/huggingface_sft/lora_multirule_v2 --save_steps 100

# python main_nopartial.py --max_grad_norm 0.2 --lr_scheduler_type cosine --num_generations 12 --gradient_accumulation_steps 4 --learning_rate 5e-5 --num_train_epochs 1 --dataset tomg-group-umd/compliance --subset wildguard --model_run_name Meta-Llama-3.1-8B_wildguard --model_name /fs/cml-projects/guardian_models/models/Meta-Llama-3.1-8B-Instruct/huggingface_grpo/lora_multirule_v2 --save_steps 100 --resume_from_checkpoint
# python main_nopartial.py --max_grad_norm 0.2 --lr_scheduler_type cosine --num_generations 12 --gradient_accumulation_steps 4 --learning_rate 5e-5 --num_train_epochs 1 --dataset tomg-group-umd/compliance --subset wildguard --model_run_name Qwen2.5-14B_wildguard       --model_name /fs/cml-projects/guardian_models/models/Qwen2.5-14B-Instruct/huggingface_grpo/lora_multirule_v2       --save_steps 50 --gpu_memory_utilization 0.7 --resume_from_checkpoint
# python main_nopartial.py --max_grad_norm 0.2 --lr_scheduler_type cosine --num_generations 12 --gradient_accumulation_steps 4 --learning_rate 5e-5 --num_train_epochs 1 --dataset tomg-group-umd/compliance --subset wildguard --model_run_name Qwen2.5-7B_wildguard        --model_name /fs/cml-projects/guardian_models/models/Qwen2.5-7B-Instruct/huggingface_grpo/lora_multirule_v2        --save_steps 100 --resume_from_checkpoint
# python main_nopartial.py --max_grad_norm 0.2 --lr_scheduler_type cosine --num_generations 12 --gradient_accumulation_steps 4 --learning_rate 5e-5 --num_train_epochs 1 --dataset tomg-group-umd/compliance --subset wildguard --model_run_name Qwen2.5-3B_wildguard        --model_name /fs/cml-projects/guardian_models/models/Qwen2.5-3B-Instruct/huggingface_grpo/lora_multirule_v2          --resume_from_checkpoint
python main_nopartial.py --max_grad_norm 0.2 --lr_scheduler_type cosine --num_generations 12 --gradient_accumulation_steps 4 --learning_rate 5e-5 --num_train_epochs 1 --dataset tomg-group-umd/compliance --subset wildguard --model_run_name Qwen2.5-1.5B_wildguard      --model_name /fs/cml-projects/guardian_models/models/Qwen2.5-1.5B-Instruct/huggingface_grpo/lora_multirule_v2        --resume_from_checkpoint
python main_nopartial.py --max_grad_norm 0.2 --lr_scheduler_type cosine --num_generations 12 --gradient_accumulation_steps 4 --learning_rate 5e-5 --num_train_epochs 1 --dataset tomg-group-umd/compliance --subset wildguard --model_run_name Qwen2.5-0.5B_wildguard      --model_name /fs/cml-projects/guardian_models/models/Qwen2.5-0.5B-Instruct/huggingface_grpo/lora_multirule_v2        --resume_from_checkpoint
